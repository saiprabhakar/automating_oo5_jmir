{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import openai\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "\n",
    "import constants\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cred.json', 'r') as f:\n",
    "    cred_json = json.load(f)\n",
    "\n",
    "openai.api_key = cred_json['api_key']\n",
    "openai.organization_id = cred_json['organization_id']\n",
    "# list_models = openai.Model.list()\n",
    "# [x.id for x in list_models['data'] if 'gpt' in x.id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Good morning, Ms. Smith!',\n",
       "  'How are you feeling today?',\n",
       "  'Morning, Doctor.'],\n",
       " [0, 1, 2],\n",
       " [12, 13, 14, 17, 18, 30])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract conversations and gt\n",
    "example_sentences, example_sentences_idx, gt_idx = [], [], []\n",
    "with open('example_conv_sents.txt', 'r') as f:\n",
    "    file_lines = f.read().splitlines()\n",
    "    for x in file_lines:\n",
    "        x = x.strip()\n",
    "\n",
    "        if not x:\n",
    "            continue\n",
    "        if x.startswith(\"gt\"):\n",
    "            idx = int(x[3:].split(\":\")[0])\n",
    "            sent = \":\".join(x[3:].strip().split(\":\")[1:]).strip()\n",
    "            gt_idx.append(idx)\n",
    "        else:\n",
    "            idx = int(x.split(\":\")[0])\n",
    "            sent = \":\".join(x.split(\":\")[1:]).strip()\n",
    "\n",
    "        example_sentences.append(sent)\n",
    "        example_sentences_idx.append(idx)\n",
    "\n",
    "example_sentences[:3], example_sentences_idx[:3], gt_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and save outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running gpt-3.5-turbo-0301\n",
      "running gpt-3.5-turbo-0301\n",
      "running gpt-3.5-turbo-0301\n",
      "running gpt-3.5-turbo-0301\n"
     ]
    }
   ],
   "source": [
    "fileid = \"example\"\n",
    "response_all = defaultdict(list)\n",
    "\n",
    "# (model, use_few_shot, use_explanation)\n",
    "eval_options = [\n",
    "    ('gpt-3.5-turbo-0301', True, True),\n",
    "    ('gpt-3.5-turbo-0301', False, True),\n",
    "    ('gpt-3.5-turbo-0301', False, False),\n",
    "    ('gpt-3.5-turbo-0301', True, False),\n",
    "    # ('text-bison@001', True, True),\n",
    " ]\n",
    "tag = f'_1'\n",
    "\n",
    "for model, use_few_shot, use_explanation in eval_options:\n",
    "    print('running', model)\n",
    "\n",
    "    # batch the examples into batches of BATCH_SIZE\n",
    "    idx = list(range(0, len(example_sentences), constants.BATCH_SIZE))\n",
    "    example_sentences_idx = list(range(len(example_sentences)))\n",
    "\n",
    "    # predict for each batch\n",
    "    for batchid, (st, end) in enumerate(zip(idx[:], idx[1:] + [len(example_sentences)])):\n",
    "        utterances = example_sentences[st:end]\n",
    "        utterances_ids = example_sentences_idx[st:end]\n",
    "        \n",
    "        # Create a list of message objects - input to the LLM model\n",
    "        start_offset = list(utterances_ids)[0] if constants.USE_ZERO_ALL_BATCHES else 0\n",
    "        messages = utils.get_messages(utterances, [x-start_offset for x in utterances_ids],\n",
    "                                use_few_shot=use_few_shot,\n",
    "                                example_inputs=constants.example_inputs,\n",
    "                                example_outputs=constants.example_outputs, use_explanation=use_explanation)\n",
    "        \n",
    "        # get predictions for OpenAI and Google models\n",
    "        # process google model's input differently as per their documentation\n",
    "        if 'bison' in model:\n",
    "            google_prompt = get_google_prompt(messages)\n",
    "            parameters = {\n",
    "                \"max_output_tokens\": 256,\n",
    "                \"temperature\": 0.0,\n",
    "                \"top_p\": 1,\n",
    "                \"top_k\": 40\n",
    "            }\n",
    "            google_model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "            response = google_model.predict(\n",
    "                google_prompt,\n",
    "                **parameters\n",
    "            )\n",
    "\n",
    "            response_all[f\"{fileid}_{tag}_{model}_fewshot_{use_few_shot}_useexp_{use_explanation}\"].append({\n",
    "                'vendor': 'google', 'utterances_ids': utterances_ids, 'response': response,\n",
    "                \"messages\": messages, \"google_prompt\": google_prompt, \"model\": model, 'start_idx': start_offset})\n",
    "        else:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=0.0,\n",
    "                top_p=1,\n",
    "            )\n",
    "        # store the LLM predictions\n",
    "        response_all[f\"{fileid}_{tag}_{model}_fewshot_{use_few_shot}_useexp_{use_explanation}\"].append({\n",
    "            'utterances_ids': utterances_ids, 'response': response, \"messages\": messages, 'start_offset': start_offset})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo-0301 True True\n",
      "pred_sentence_ids_list_all [10, 11, 12, 14, 16, 17, 18, 21, 22, 24, 25, 26] gt_idx [12, 13, 14, 17, 18, 30]\n",
      "clusters {'tp_clusters_in_gt_expanded': [[11, 19]], 'tp_clusters_in_pred': [[10, 18]], 'fn_clusters_in_pred': [[21, 26]], 'fn_clusters_in_gt_expanded': [[29, 31]], 'fp_clusters_in_pred': [[21, 26]]}\n",
      "metrics {'precision': 0.5, 'recall': 0.5, 'true_positives': 1, 'false_positives': 1, 'false_negatives': 1}\n",
      "\n",
      "gpt-3.5-turbo-0301 False True\n",
      "pred_sentence_ids_list_all [10, 14, 15, 17, 18, 21, 22, 24, 25, 26] gt_idx [12, 13, 14, 17, 18, 30]\n",
      "clusters {'tp_clusters_in_gt_expanded': [[11, 19]], 'tp_clusters_in_pred': [[14, 18]], 'fn_clusters_in_pred': [[21, 26]], 'fn_clusters_in_gt_expanded': [[29, 31]], 'fp_clusters_in_pred': [[10, 10], [21, 26]]}\n",
      "metrics {'precision': 0.3333333333333333, 'recall': 0.5, 'true_positives': 1, 'false_positives': 2, 'false_negatives': 1}\n",
      "\n",
      "gpt-3.5-turbo-0301 False False\n",
      "pred_sentence_ids_list_all [10, 14, 15, 17, 18, 21, 22, 24, 25, 26] gt_idx [12, 13, 14, 17, 18, 30]\n",
      "clusters {'tp_clusters_in_gt_expanded': [[11, 19]], 'tp_clusters_in_pred': [[14, 18]], 'fn_clusters_in_pred': [[21, 26]], 'fn_clusters_in_gt_expanded': [[29, 31]], 'fp_clusters_in_pred': [[10, 10], [21, 26]]}\n",
      "metrics {'precision': 0.3333333333333333, 'recall': 0.5, 'true_positives': 1, 'false_positives': 2, 'false_negatives': 1}\n",
      "\n",
      "gpt-3.5-turbo-0301 True False\n",
      "pred_sentence_ids_list_all [10, 11, 12, 14, 16, 17, 18, 21, 22, 24, 25, 26] gt_idx [12, 13, 14, 17, 18, 30]\n",
      "clusters {'tp_clusters_in_gt_expanded': [[11, 19]], 'tp_clusters_in_pred': [[10, 18]], 'fn_clusters_in_pred': [[21, 26]], 'fn_clusters_in_gt_expanded': [[29, 31]], 'fp_clusters_in_pred': [[21, 26]]}\n",
      "metrics {'precision': 0.5, 'recall': 0.5, 'true_positives': 1, 'false_positives': 1, 'false_negatives': 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_metric = []\n",
    "for model, use_few_shot, use_explanation in eval_options:\n",
    "    print(model, use_few_shot, use_explanation)\n",
    "    metrics_all = defaultdict(list)\n",
    "\n",
    "    llm_out = response_all[f\"{fileid}_{tag}_{model}_fewshot_{use_few_shot}_useexp_{use_explanation}\"]\n",
    "    \n",
    "    # parse and get the valid sentence ids from the LLM predictions\n",
    "    pred_sentence_ids_list_all = []\n",
    "    for llm_response in llm_out:\n",
    "        # get the predictions for the LLM models\n",
    "        if 'bison' in model:\n",
    "            out = llm_response['response'].text.strip()\n",
    "        else:\n",
    "            out = llm_response['response'].choices[0].message[\"content\"].strip()\n",
    "\n",
    "        # parse the predictions to get the sentence ids\n",
    "        pred_sentence_ids_list = utils.parse_and_get_utt_ids_v3(out)\n",
    "\n",
    "        # get the start offset if it's not zero\n",
    "        start_offset = llm_response.get('start_offset', 0)\n",
    "        pred_sentence_ids_list = [int(start_offset + x) for x in pred_sentence_ids_list]\n",
    "        \n",
    "        # check if the sentence ids are valid - if it is in the rane of the input sentences\n",
    "        input_utterances_ids = [int(x) for x in set(llm_response['utterances_ids'])]\n",
    "        not_found = []\n",
    "        for sentence_id in pred_sentence_ids_list:\n",
    "            if sentence_id not in input_utterances_ids:\n",
    "                print(\"Sentence ID {} not found in utterances\".format(sentence_id))\n",
    "                not_found.append(sentence_id)\n",
    "        for id in not_found:\n",
    "            pred_sentence_ids_list.remove(id)\n",
    "        pred_sentence_ids_list_all.extend(pred_sentence_ids_list)\n",
    "\n",
    "    # get the TP, FP, FN clusters for the predictions\n",
    "    pred_array = [1 if x in pred_sentence_ids_list_all else 0 for x in example_sentences_idx]\n",
    "    pred_sentence_ids_list_all = list(set(pred_sentence_ids_list_all))\n",
    "    metrics, clusters = utils.get_metrics(gt_idx, pred_sentence_ids_list_all)\n",
    "\n",
    "    for k, v in metrics.items():\n",
    "        metrics_all[k].append(v)\n",
    "\n",
    "    # calculate precision, recall and f1\n",
    "    precisiondenominator = max(1, sum(metrics_all['true_positives']) + sum(metrics_all['false_positives']))\n",
    "    recalldenominator = max(1, sum(metrics_all['true_positives']) + sum(metrics_all['false_negatives']))\n",
    "    precision = sum(metrics_all['true_positives'])/precisiondenominator\n",
    "    recall = sum(metrics_all['true_positives'])/recalldenominator\n",
    "    f1 = 2*precision*recall/max(precision+recall, 1)\n",
    "\n",
    "    display_metric.append({\n",
    "        'tag': tag,\n",
    "        'model':model, 'use_few_shot': use_few_shot,\n",
    "        'precision': precision, 'recall': recall, 'f1': f1,\n",
    "        'num_file': 1, 'use_explanation': use_explanation\n",
    "    })\n",
    "\n",
    "    print('pred_sentence_ids_list_all', pred_sentence_ids_list_all, 'gt_idx', gt_idx)\n",
    "    print('clusters', clusters)\n",
    "    print('metrics', metrics)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>model</th>\n",
       "      <th>use_few_shot</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>num_file</th>\n",
       "      <th>use_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_1</td>\n",
       "      <td>gpt-3.5-turbo-0301</td>\n",
       "      <td>True</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_1</td>\n",
       "      <td>gpt-3.5-turbo-0301</td>\n",
       "      <td>False</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_1</td>\n",
       "      <td>gpt-3.5-turbo-0301</td>\n",
       "      <td>False</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1</td>\n",
       "      <td>gpt-3.5-turbo-0301</td>\n",
       "      <td>True</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tag               model  use_few_shot  precision  recall        f1  \\\n",
       "0  _1  gpt-3.5-turbo-0301          True   0.500000     0.5  0.500000   \n",
       "1  _1  gpt-3.5-turbo-0301         False   0.333333     0.5  0.333333   \n",
       "2  _1  gpt-3.5-turbo-0301         False   0.333333     0.5  0.333333   \n",
       "3  _1  gpt-3.5-turbo-0301          True   0.500000     0.5  0.500000   \n",
       "\n",
       "   num_file  use_explanation  \n",
       "0         1             True  \n",
       "1         1             True  \n",
       "2         1            False  \n",
       "3         1            False  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df = pd.DataFrame(display_metric)\n",
    "metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
